---
alwaysApply: true
---

# SpaceONE Google Cloud Inventory Collector ë¡œê¹… í‘œì¤€

## ë¡œê¹…ì˜ í•„ìš”ì„±

-   **ë¬¸ì œ ì§„ë‹¨**: Google Cloud API ì˜¤ë¥˜ ì›ì¸ íŒŒì•…, ìˆ˜ì§‘ ì„±ëŠ¥ ë³‘ëª© ì‹ë³„
-   **ë³´ì•ˆ**: ì¸ì¦ ì‹œë„ íƒì§€, Google Cloud ë¦¬ì†ŒìŠ¤ ì ‘ê·¼ ê°ì‚¬
-   **ìš´ì˜ ëª¨ë‹ˆí„°ë§**: ì¸ë²¤í† ë¦¬ ìˆ˜ì§‘ ìƒíƒœ, í”ŒëŸ¬ê·¸ì¸ ì„±ëŠ¥ ì¶”ì 

### âš ï¸ print() ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€

```python
# âŒ ì ˆëŒ€ ê¸ˆì§€
print(f"Collecting clusters from project {project_id}")

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
logger.info(f"Collecting clusters from project {project_id}")
```

## ë¡œê¹… ë ˆë²¨ ì´í•´í•˜ê¸°

### ë¡œê¹… ë ˆë²¨ë³„ ì‚¬ìš© ëª©ì  (SpaceONE í”ŒëŸ¬ê·¸ì¸ íŠ¹í™”)

ê° ë¡œê¹… ë ˆë²¨ì€ **ëª…í™•í•œ ëª©ì **ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

| ë ˆë²¨         | ëª©ì                    | ì–¸ì œ ì‚¬ìš©í• ê¹Œ?         | SpaceONE í”ŒëŸ¬ê·¸ì¸ ì˜ˆì‹œ                     |
| ------------ | ---------------------- | ---------------------- | ------------------------------------------ |
| **DEBUG**    | ìƒì„¸í•œ ê°œë°œ ì •ë³´       | ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œë§Œ | API ìš”ì²­/ì‘ë‹µ ìƒì„¸, ë°ì´í„° ë³€í™˜ ê³¼ì •       |
| **INFO**     | ì •ìƒì ì¸ í”„ë¡œì„¸ìŠ¤ íë¦„ | ì¤‘ìš”í•œ ìˆ˜ì§‘ ì´ë²¤íŠ¸     | ìˆ˜ì§‘ ì‹œì‘/ì™„ë£Œ, ë¦¬ì†ŒìŠ¤ ë°œê²¬, ì¸ì¦ ì„±ê³µ     |
| **WARNING**  | ì˜ˆìƒ ê°€ëŠ¥í•œ ë¬¸ì œ       | ë³µêµ¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ìƒí™©  | API í• ë‹¹ëŸ‰ ê²½ê³ , ë¦¬ì „ ì ‘ê·¼ ì œí•œ            |
| **ERROR**    | ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì˜¤ë¥˜     | ê¸°ëŠ¥ ì‹¤í–‰ ì‹¤íŒ¨         | ì¸ì¦ ì‹¤íŒ¨, API í˜¸ì¶œ ì˜¤ë¥˜, ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨ |
| **CRITICAL** | ì„œë¹„ìŠ¤ ì¤‘ë‹¨ê¸‰ ì˜¤ë¥˜     | ì‹œìŠ¤í…œ ì „ì²´ ì¥ì•        | í”ŒëŸ¬ê·¸ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨, ì¹˜ëª…ì  ì„¤ì • ì˜¤ë¥˜     |

### í™˜ê²½ë³„ ë¡œê¹… ë ˆë²¨ ì„¤ì •

```
ë¡œì»¬ ê°œë°œí™˜ê²½: DEBUG ì´ìƒ (ìƒì„¸í•œ API ë””ë²„ê¹…)
SpaceONE í…ŒìŠ¤íŠ¸í™˜ê²½: INFO ì´ìƒ (ìˆ˜ì§‘ í”Œë¡œìš° ì¶”ì )
SpaceONE ìŠ¤í…Œì´ì§•í™˜ê²½: INFO ì´ìƒ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§)
SpaceONE ìš´ì˜í™˜ê²½: WARNING ì´ìƒ (ì˜¤ë¥˜ ë° ê²½ê³ ë§Œ)
```

## ë¬´ì—‡ì„ ì–¸ì œ ë¡œê·¸í•´ì•¼ í•˜ëŠ”ê°€?

### í•„ìˆ˜ ë¡œê¹… ëŒ€ìƒ (Google Cloud ìˆ˜ì§‘ê¸° íŠ¹í™”)

**1. ì¸ì¦ ë° ê¶Œí•œ ê´€ë ¨**

-   Google Cloud Service Account ì¸ì¦ ì‹œë„ (ì„±ê³µ/ì‹¤íŒ¨)
-   í”„ë¡œì íŠ¸ë³„ ê¶Œí•œ í™•ì¸ ê²°ê³¼
-   API í‚¤ ê°±ì‹  ë° ë§Œë£Œ

**2. ì¤‘ìš”í•œ ìˆ˜ì§‘ ì´ë²¤íŠ¸**

-   ìˆ˜ì§‘ ì‘ì—… ì‹œì‘/ì™„ë£Œ
-   ë¦¬ì†ŒìŠ¤ ë°œê²¬ ë° ë¶„ë¥˜
-   ìƒˆë¡œìš´ ë¦¬ì†ŒìŠ¤ ìœ í˜• ê°ì§€

**3. Google Cloud API ìƒíƒœ**

-   API ì‘ë‹µ ì‹œê°„ ë° ìƒíƒœ
-   í• ë‹¹ëŸ‰ ì‚¬ìš©ëŸ‰ ë° ì œí•œ
-   ë¦¬ì „ë³„ ê°€ìš©ì„± í™•ì¸

**4. ì˜¤ë¥˜ ë° ì˜ˆì™¸ ìƒí™©**

-   API í˜¸ì¶œ ì‹¤íŒ¨ ë° ì¬ì‹œë„
-   ë°ì´í„° ë³€í™˜ ì˜¤ë¥˜
-   ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ

### ë¡œê¹…í•˜ì§€ ë§ì•„ì•¼ í•  ë°ì´í„°

**ì ˆëŒ€ ë¡œê¹… ê¸ˆì§€ í•­ëª©:**

-   **ì¸ì¦ ì •ë³´**: Service Account í‚¤, ì•¡ì„¸ìŠ¤ í† í° ì›ë¬¸
-   **ë¯¼ê°í•œ ë¦¬ì†ŒìŠ¤ ì •ë³´**: ë‚´ë¶€ IP, ë³´ì•ˆ ê·¸ë£¹ ì„¸ë¶€ì‚¬í•­
-   **ê°œì¸ì •ë³´**: ì‚¬ìš©ì ì‹ë³„ ì •ë³´, ì´ë©”ì¼
-   **ê¸°ë°€ ì •ë³´**: í”„ë¡œì íŠ¸ ë‚´ë¶€ êµ¬ì¡°, ë³´ì•ˆ ì •ì±…

**âš ï¸ ë³´ì•ˆ ìœ„í—˜ ì˜ˆì‹œ:**

```python
# âŒ ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ
logger.info(f"Service account key: {service_account_key}")
logger.debug(f"Access token: {access_token}")
logger.info(f"Internal IP: {instance.internal_ip}")

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
logger.info("Service account authentication successful")
logger.debug("Access token refreshed successfully")
logger.info(f"Instance discovered: {instance.name} in zone {instance.zone}")
```

## êµ¬ì¡°í™”ëœ ë¡œê¹…

### JSON í˜•íƒœ ë¡œê¹… ì„¤ì •

```python
import logging
import json
from datetime import datetime

class SpaceONEJSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "plugin": "google-cloud-inventory-collector",
            "service": getattr(record, 'service', 'unknown')
        }
        return json.dumps(log_data, ensure_ascii=False)

def setup_logging():
    logger = logging.getLogger()
    handler = logging.StreamHandler()
    handler.setFormatter(SpaceONEJSONFormatter())
    logger.addHandler(handler)
```

### ì‹¤ìš©ì  ë¡œê¹… ë°©ë²•

**ê°„ë‹¨í•œ ë©”ì‹œì§€ ë‚´ í¬í•¨ ë°©ì‹ ê¶Œì¥:**

```python
# âœ… ê¶Œì¥: ë©”ì‹œì§€ ë‚´ ì§ì ‘ í¬í•¨
logger.info(f"Collected {cluster_count} Dataproc clusters from project {project_id}")
logger.warning(f"API quota 80% reached for project {project_id}")
logger.error(f"Failed to connect to region {region}: {error}")

# ë³µì¡í•œ extra ì‚¬ìš©ì€ íŠ¹ë³„í•œ ê²½ìš°ì—ë§Œ
logger.info("Critical collection event", extra={'event_type': 'quota_exceeded', 'project': project_id})
```

## ë ˆì´ì–´ë³„ ë¡œê¹… ì „ëµ (SpaceONE í”ŒëŸ¬ê·¸ì¸ êµ¬ì¡°)

### 1. Service ë ˆì´ì–´ (spaceone/inventory/service/collector_service.py)

- í”ŒëŸ¬ê·¸ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ë° ìˆ˜ì§‘ ì‘ì—… ì „ì²´ í”Œë¡œìš°
- ì¸ì¦ ë° ê¶Œí•œ ê²€ì¦
- ì „ì²´ ìˆ˜ì§‘ ì„±ëŠ¥ ë° ê²°ê³¼ ìš”ì•½
- ìƒíƒœ ì¹´ìš´í„° ì´ˆê¸°í™” ë° ìµœì¢… ìš”ì•½ ë¡œê¹…

```python
import logging
import time
from spaceone.core.service import BaseService
from spaceone.inventory.manager.dataproc.cluster_manager import DataprocClusterManager
from spaceone.inventory.libs.schema.base import (
    reset_state_counters,
    log_state_summary
)

logger = logging.getLogger(__name__)

class CollectorService(BaseService):
    def collect_cloud_service(self, secret_data, options, **kwargs):
        start_time = time.time()
        
        # ìƒíƒœ ì¹´ìš´í„° ì´ˆê¸°í™”
        reset_state_counters()
        
        logger.info(f"Starting Google Cloud Dataproc collection for project {secret_data.get('project_id')}")
        
        try:
            cluster_manager = DataprocClusterManager()
            resources = cluster_manager.collect_resources(secret_data, options)
            
            # ìµœì¢… ìš”ì•½ ì •ë³´ ë¡œê¹…
            log_state_summary()
            logger.info(f"Successfully collected {len(resources)} Dataproc clusters in {time.time() - start_time:.2f}s")
            return resources
        except Exception as e:
            logger.error(f"Failed to collect Dataproc resources: {str(e)}")
            raise
    
    @staticmethod
    def generate_error_response(e, cloud_service_group, cloud_service_type):
        """
        ê°œì„ ëœ ë¡œê¹… ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì—ëŸ¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        from spaceone.inventory.libs.schema.cloud_service import ErrorResourceResponse
        import json
        
        if type(e) is dict:
            error_message = json.dumps(e)
            error_code = "DICT_ERROR"
        else:
            error_message = str(e)
            error_code = type(e).__name__
        
        # ë¡œê¹…ê³¼ í•¨ê»˜ ì—ëŸ¬ ì‘ë‹µ ìƒì„±
        return ErrorResourceResponse.create_with_logging(
            error_message=error_message,
            error_code=error_code,
            additional_data={
                "cloud_service_group": cloud_service_group,
                "cloud_service_type": cloud_service_type,
            }
        )
```

### 2. Manager ë ˆì´ì–´ (spaceone/inventory/manager/dataproc/cluster_manager.py)

- ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰
- ë°ì´í„° ë³€í™˜ ë° ê²€ì¦
- Connector í˜¸ì¶œ ê²°ê³¼ ì²˜ë¦¬

```python
import logging
from spaceone.core.manager import BaseManager
from spaceone.inventory.connector.dataproc.cluster_connector import DataprocClusterConnector

logger = logging.getLogger(__name__)

class DataprocClusterManager(BaseManager):
    def collect_resources(self, secret_data, options):
        logger.info("Starting Dataproc cluster collection")
        
        connector = DataprocClusterConnector()
        connector.set_secret_data(secret_data)
        
        try:
            # ë¦¬ì „ë³„ ìˆœì°¨ ìˆ˜ì§‘
            regions = connector.list_regions()
            logger.debug(f"Found {len(regions)} regions for Dataproc collection")
            
            resources = []
            for region in regions:
                clusters = connector.list_clusters(region)
                logger.debug(f"Found {len(clusters)} clusters in region {region}")
                resources.extend(self._convert_clusters_to_resources(clusters))
            
            logger.info(f"Collected {len(resources)} total Dataproc clusters")
            return resources
            
        except Exception as e:
            logger.error(f"Error during cluster collection: {str(e)}")
            raise
```

### 3. Connector ë ˆì´ì–´ (spaceone/inventory/connector/dataproc/cluster_connector.py)

- Google Cloud API í˜¸ì¶œ
- ì™¸ë¶€ API ì‘ë‹µ ì²˜ë¦¬
- ìˆœì°¨ ì²˜ë¦¬ ë° ì•ˆì •ì„±
- ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë° ì¬ì‹œë„ ë¡œì§

```python
import logging
import time
import socket
import ssl
# ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ë¨ - ThreadPoolExecutor ì‚¬ìš© ì•ˆ í•¨
from googleapiclient.errors import HttpError
from spaceone.core.connector import BaseConnector

logger = logging.getLogger(__name__)

class DataprocClusterConnector(BaseConnector):
    def list_clusters(self, **query):
        """ìˆœì°¨ ì²˜ë¦¬ë¥¼ í†µí•œ ëª¨ë“  ë¦¬ì „ì˜ í´ëŸ¬ìŠ¤í„° ì¡°íšŒ"""
        if query.get("region"):
            # íŠ¹ì • ë¦¬ì „ ì¡°íšŒ
            return self._list_single_region_clusters(query["region"], **query)
        else:
            # ëª¨ë“  ë¦¬ì „ ë³‘ë ¬ ì¡°íšŒ
            return self._list_clusters_sequential(**query)
    
    def _list_clusters_parallel(self, **query):
        """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ëª¨ë“  ë¦¬ì „ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        regions = self._get_optimized_regions()
        cluster_list = []
        
        # ë©”ëª¨ë¦¬ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ€ 3ê°œ ì›Œì»¤ë¡œ ì œí•œ
        max_workers = min(3, len(regions))
        
        logger.info(f"Starting parallel cluster collection across {len(regions)} regions with {max_workers} workers")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_region = {
                executor.submit(self._list_clusters_in_region, region, **query): region
                for region in regions
            }
            
            try:
                for future in as_completed(future_to_region, timeout=90):
                    region = future_to_region[future]
                    try:
                        clusters = future.result(timeout=60)
                        if clusters:
                            cluster_list.extend(clusters)
                            logger.debug(f"Found {len(clusters)} clusters in region {region}")
                    except Exception as e:
                        logger.debug(f"Error processing region {region}: {e}")
                        continue
                        
            except Exception as e:
                logger.warning(f"Timeout waiting for region processing: {e}")
        
        logger.info(f"Parallel collection completed: {len(cluster_list)} total clusters")
        return cluster_list
    
    def _list_clusters_in_region(self, region, **query):
        """íŠ¹ì • ë¦¬ì „ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì¡°íšŒ (ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                # ìŠ¤ë ˆë“œë³„ ë…ë¦½ì ì¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                client = self._get_thread_safe_client()
                if not client:
                    logger.warning(f"No client available for region {region}")
                    return []
                
                request = client.projects().regions().clusters().list(
                    projectId=self.project_id, region=region, **query
                )
                response = request.execute()
                return response.get("clusters", [])
                
            except HttpError as e:
                if e.resp.status in [404, 403]:
                    return []
                elif e.resp.status == 429:
                    wait_time = retry_delay * (2**attempt)
                    logger.warning(f"Rate limit in region {region}, waiting {wait_time}s")
                    time.sleep(wait_time)
                    continue
                elif e.resp.status >= 500 and attempt < max_retries - 1:
                    logger.warning(f"Server error in region {region}, retrying...")
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    logger.warning(f"HTTP error in region {region}: {e}")
                    return []
                    
            except (ConnectionError, TimeoutError, socket.timeout, ssl.SSLError) as e:
                if attempt < max_retries - 1:
                    logger.warning(f"Network/SSL error in region {region}, retrying (attempt {attempt + 1}): {e}")
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    logger.warning(f"Network/SSL error in region {region} after {max_retries} attempts: {e}")
                    return []
                    
            except Exception as e:
                logger.debug(f"Unexpected error in region {region}: {e}")
                return []
        
        return []
```

### 4. ìˆœì°¨ ì²˜ë¦¬ ë¡œê¹… íŒ¨í„´ (v3.0) - ì•ˆì •ì„± ìµœì í™”

ìˆœì°¨ ì²˜ë¦¬ ì‹œìŠ¤í…œì—ì„œëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì•ˆì •ì„±ì„ ìš°ì„ ì‹œí•˜ë©°, ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ìƒíƒœë¥¼ ìƒì„¸íˆ ë¡œê¹…í•©ë‹ˆë‹¤:

```python
import logging
import time
# ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ë¨ - ThreadPoolExecutor ì‚¬ìš© ì•ˆ í•¨

logger = logging.getLogger(__name__)

def _list_clusters_parallel(self, **query):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ì§‘ (ìƒì„¸ ë¡œê¹… í¬í•¨)"""
    start_time = time.time()
    regions = self._get_optimized_regions()
    cluster_list = []
    
    # ê³ ì„±ëŠ¥ ì›Œì»¤ ìˆ˜ ë° íƒ€ì„ì•„ì›ƒ ì„¤ì • (ìµœì í™”ë¨)
    max_workers = min(12, len(regions))  # ìµœê³  ì„±ëŠ¥ì„ ìœ„í•œ 12ê°œ ì›Œì»¤
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ë¡œê¹… (ìµœì í™”ëœ ì„¤ì • ì •ë³´ í¬í•¨)
    logger.info(
        f"ğŸš€ Starting parallel cluster collection: "
        f"regions={len(regions)}, max_workers={max_workers}, "
        f"global_timeout=90s, individual_timeout=60s (optimized for 12 workers)"
    )
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_region = {
            executor.submit(self._list_clusters_in_region, region, **query): region
            for region in regions
        }
        
        try:
            for future in as_completed(future_to_region, timeout=90):
                region = future_to_region[future]
                try:
                    clusters = future.result(timeout=60)
                    if clusters:
                        cluster_list.extend(clusters)
                        logger.debug(f"Found {len(clusters)} clusters in region {region}")
                except Exception as e:
                    logger.debug(f"Error processing region {region}: {e}")
                    continue
                    
        except Exception as e:
            logger.warning(f"Timeout waiting for region processing: {e}")
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ ë¡œê¹… (ì„±ëŠ¥ ë©”íŠ¸ë¦­ í¬í•¨)
    execution_time = time.time() - start_time
    logger.info(
        f"âœ… Parallel cluster collection completed: "
        f"total_clusters={len(cluster_list)}, "
        f"processed_regions={len(regions)}, "
        f"execution_time={execution_time:.2f}s, "
        f"avg_time_per_region={execution_time / len(regions):.2f}s, "
        f"throughput={len(cluster_list)/execution_time:.1f} clusters/sec"
    )
    
    return cluster_list

def _list_jobs_parallel(self, **query):
    """ë³‘ë ¬ ì‘ì—… ìˆ˜ì§‘ (ìµœì í™”ëœ ë¡œê¹…)"""
    start_time = time.time()
    regions = self._get_optimized_regions()
    job_list = []
    
    # ì‘ì—… ìˆ˜ì§‘ìš© ìµœì í™”ëœ ì›Œì»¤ ìˆ˜ (6ê°œë¡œ ì¦ê°€)
    max_workers = min(6, len(regions))  # ìµœê³  ì„±ëŠ¥ì„ ìœ„í•œ 6ê°œ ì›Œì»¤
    
    logger.info(
        f"âš¡ Starting parallel job collection: "
        f"regions={len(regions)}, max_workers={max_workers}, "
        f"individual_timeout=15s (optimized for 6 workers)"
    )
    
    # ... ì²˜ë¦¬ ë¡œì§ ...
    
    execution_time = time.time() - start_time
    logger.info(
        f"âš¡ Parallel job collection completed: "
        f"total_jobs={len(job_list)}, "
        f"processed_regions={len(regions)}, "
        f"execution_time={execution_time:.2f}s, "
        f"throughput={len(job_list)/max(execution_time, 0.001):.1f} jobs/sec"
    )
    
    return job_list
```

#### ë³‘ë ¬ ì²˜ë¦¬ ë¡œê¹… ê°€ì´ë“œë¼ì¸ (v2.0 ê³ ì„±ëŠ¥ ìµœì í™”)

**ì‹œì‘ ë¡œê¹… (INFO ë ˆë²¨)**:
- ê³ ì„±ëŠ¥ ì›Œì»¤ ìˆ˜ (`max_workers=12` for clusters, `max_workers=6` for jobs)
- ì²˜ë¦¬ ëŒ€ìƒ ìˆ˜ (`regions=N`)
- ì°¨ë“± íƒ€ì„ì•„ì›ƒ ì„¤ì • (`global_timeout=90s, individual_timeout=60s/15s`)
- ìµœì í™” ì •ë³´ (`optimized for 12 workers` / `optimized for 6 workers`)

**ì™„ë£Œ ë¡œê¹… (INFO ë ˆë²¨)**:
- ì´ ìˆ˜ì§‘ ê²°ê³¼ (`total_clusters=N, total_jobs=N`)
- ì²˜ë¦¬ëœ ë¦¬ì „ ìˆ˜ (`processed_regions=N`)
- ì‹¤í–‰ ì‹œê°„ (`execution_time=N.NNs`)
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ (`throughput=N.N items/sec`)
- í‰ê·  ì‹œê°„ (`avg_time_per_region=N.NNs`)

**ê°œë³„ ë¦¬ì „ ë¡œê¹… (DEBUG ë ˆë²¨)**:
- ë¦¬ì „ë³„ ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ
- ë¦¬ì „ë³„ ìˆ˜ì§‘ ê²°ê³¼ ìˆ˜

**ì—ëŸ¬ ë¡œê¹… (WARNING/DEBUG ë ˆë²¨)**:
- íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ WARNING
- ê°œë³„ ë¦¬ì „ ì‹¤íŒ¨ ì‹œ DEBUG

## ë™ì  ë¦¬ì „ ì¡°íšŒ ë¡œê¹… íŒ¨í„´ (v2.0)

### Google Cloud Compute APIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ë¦¬ì „ ì¡°íšŒ

ìƒˆë¡œìš´ ë™ì  ë¦¬ì „ ì¡°íšŒ ì‹œìŠ¤í…œì€ Google Cloud Compute APIë¥¼ í†µí•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ì „ ëª©ë¡ì„ ì¡°íšŒí•˜ê³ , ì‹¤íŒ¨ ì‹œ fallback ë¦¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
import logging
import googleapiclient.discovery

logger = logging.getLogger(__name__)

def _get_optimized_regions(self):
    """ìµœì í™”ëœ ë¦¬ì „ ëª©ë¡ ë°˜í™˜ (ìºì‹œ ë° ë™ì  ì¡°íšŒ í¬í•¨)"""
    current_time = time.time()
    
    # ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ (5ë¶„ TTL)
    if (self._regions_cache is not None and 
        current_time - self._cache_timestamp < self._cache_ttl):
        logger.debug(f"Using cached regions: {len(self._regions_cache)} regions")
        return self._regions_cache
    
    try:
        # ë™ì  ë¦¬ì „ ì¡°íšŒ ì‹œë„
        regions = self._fetch_dataproc_regions()
        logger.info(f"Successfully fetched {len(regions)} Dataproc regions dynamically")
    except Exception as e:
        logger.warning(f"Failed to fetch dynamic regions, using core regions: {e}")
        # í•µì‹¬ ë¦¬ì „ìœ¼ë¡œ fallback (ì„±ëŠ¥ ìµœì í™”)
        regions = self._get_core_regions()
    
    # ìºì‹œ ì—…ë°ì´íŠ¸
    self._regions_cache = regions
    self._cache_timestamp = current_time
    
    logger.debug(f"Using {len(regions)} regions for Dataproc scanning")
    return regions

def _fetch_dataproc_regions(self):
    """Google Cloud Compute APIë¥¼ í†µí•œ ë™ì  ë¦¬ì „ ì¡°íšŒ"""
    if not hasattr(self, "client") or not self.client:
        raise ValueError("Client not initialized for dynamic region fetching")
    
    try:
        # Compute Engine API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        compute_client = googleapiclient.discovery.build(
            "compute", "v1", credentials=self.credentials
        )
        request = compute_client.regions().list(project=self.project_id)
        response = request.execute()
        
        all_regions = []
        if "items" in response:
            for region in response["items"]:
                region_name = region.get("name", "")
                if region_name and region.get("status") == "UP":
                    all_regions.append(region_name)
        
        # ì•Œë ¤ì§„ Dataproc ë¯¸ì§€ì› ë¦¬ì „ ì œì™¸
        excluded_regions = {"global"}
        supported_regions = [r for r in all_regions if r not in excluded_regions]
        
        if not supported_regions:
            raise Exception("No supported regions found")
        
        logger.info(f"Dynamic region query successful: {len(supported_regions)} regions available")
        return sorted(supported_regions)
        
    except Exception as e:
        logger.error(f"Failed to fetch regions from Compute API: {e}")
        raise

def _get_core_regions(self):
    """ë™ì  ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í•µì‹¬ ë¦¬ì „ (ì„±ëŠ¥ ìµœì í™”)"""
    core_regions = [
        # ì•„ì‹œì•„ ì£¼ìš” ë¦¬ì „
        "asia-east1", "asia-northeast1", "asia-northeast3", "asia-southeast1",
        # ìœ ëŸ½ ì£¼ìš” ë¦¬ì „  
        "europe-west1", "europe-west4",
        # ë¯¸êµ­ ì£¼ìš” ë¦¬ì „
        "us-central1", "us-east1", "us-west1", "us-west2",
    ]
    logger.info(f"Using core regions for optimization: {len(core_regions)} regions")
    return core_regions
```

### ë™ì  ë¦¬ì „ ì¡°íšŒ ë¡œê¹… ê°€ì´ë“œë¼ì¸

**ì„±ê³µ ì‹œ (INFO ë ˆë²¨)**:
- `"Successfully fetched N Dataproc regions dynamically"`
- `"Dynamic region query successful: N regions available"`

**ì‹¤íŒ¨ ì‹œ (WARNING ë ˆë²¨)**:
- `"Failed to fetch dynamic regions, using core regions: {error}"`
- ìë™ìœ¼ë¡œ í•µì‹¬ ë¦¬ì „ìœ¼ë¡œ fallback ìˆ˜í–‰

**ìºì‹œ ì‚¬ìš© ì‹œ (DEBUG ë ˆë²¨)**:
- `"Using cached regions: N regions"`
- ìºì‹œ TTL(5ë¶„) ì •ë³´ í¬í•¨

**ì„±ëŠ¥ ìµœì í™” ì •ë³´ (INFO ë ˆë²¨)**:
- `"Using core regions for optimization: N regions"`
- í•µì‹¬ ë¦¬ì „ ì‚¬ìš© ì‹œ ì„±ëŠ¥ ìµœì í™” ì˜ë„ ëª…ì‹œ

## SpaceONE í”ŒëŸ¬ê·¸ì¸ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´

- SpaceONE Core í”„ë ˆì„ì›Œí¬ì˜ í‘œì¤€ ë¡œê¹… êµ¬ì¡° í™œìš©
- í”ŒëŸ¬ê·¸ì¸ë³„ ê³ ìœ  ì‹ë³„ì í¬í•¨
- ìˆ˜ì§‘ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìë™ ê¸°ë¡

```python
import logging
from spaceone.core.logger import set_logger

# SpaceONE í‘œì¤€ ë¡œê±° ì„¤ì •
set_logger('spaceone.inventory')

# í”ŒëŸ¬ê·¸ì¸ë³„ ë¡œê±° ìƒì„±
logger = logging.getLogger('spaceone.inventory.google_cloud')
```

## ìƒíƒœ ì¶”ì  ë¡œê¹… ì‹œìŠ¤í…œ (v2.0)

### ì‘ë‹µ ìƒíƒœë³„ ìë™ ì¹´ìš´í„° ë° ë¡œê¹…

ìƒˆë¡œìš´ ìƒíƒœ ì¶”ì  ì‹œìŠ¤í…œì´ ë„ì…ë˜ì–´ ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ê¸€ë¡œë²Œ ì¹´ìš´í„°ë¥¼ í†µí•´ SUCCESS, FAILURE, TIMEOUT, UNKNOWN ìƒíƒœë¥¼ ìë™ìœ¼ë¡œ ì¶”ì í•˜ê³ , ê° ìƒíƒœì— ë”°ë¼ ì ì ˆí•œ ë¡œê¹…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

```python
from spaceone.inventory.libs.schema.base import (
    BaseResponse, 
    log_state_summary, 
    reset_state_counters,
    get_state_counters
)

# ìˆ˜ì§‘ ì‹œì‘ ì‹œ ì¹´ìš´í„° ì´ˆê¸°í™”
reset_state_counters()

# ì„±ê³µ ì‘ë‹µ ìƒì„± (ìë™ ë¡œê¹…)
success_response = BaseResponse.create_with_logging(
    state="SUCCESS",
    resource_type="inventory.CloudService",
    message="Cluster collection completed",
    resource=cluster_data
)

# ì‹¤íŒ¨ ì‘ë‹µ ìƒì„± (ìë™ ì—ëŸ¬ ë¡œê¹…)
error_response = BaseResponse.create_with_logging(
    state="FAILURE", 
    resource_type="inventory.ErrorResource",
    message="Authentication failed",
)

# íƒ€ì„ì•„ì›ƒ ì‘ë‹µ ìƒì„± (ìë™ ê²½ê³  ë¡œê¹…)
timeout_response = BaseResponse.create_with_logging(
    state="TIMEOUT",
    resource_type="inventory.CloudService", 
    message="API call timeout after 90 seconds"
)

# ìˆ˜ì§‘ ì™„ë£Œ ì‹œ ìš”ì•½ ì •ë³´ ë¡œê¹…
log_state_summary()
# ì¶œë ¥ ì˜ˆì‹œ: "ğŸ“Š Response State Summary: Total=150, SUCCESS=140 (93.3%), FAILURE=8, TIMEOUT=2, UNKNOWN=0"
```

### ìƒíƒœë³„ ë¡œê¹… ë™ì‘

| ìƒíƒœ | ë¡œê¹… ë ˆë²¨ | ìë™ ë™ì‘ | ì˜ˆì‹œ |
|------|----------|----------|-----|
| **SUCCESS** | ì—†ìŒ | ì¹´ìš´í„°ë§Œ ì¦ê°€ | ì •ìƒ ì²˜ë¦¬ (ë¡œê·¸ ìŠ¤íŒ¸ ë°©ì§€) |
| **FAILURE** | ERROR | ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡ | `"Response state: FAILURE, resource_type: inventory.CloudService, message: API authentication failed"` |
| **TIMEOUT** | WARNING | ê²½ê³  ë¡œê·¸ ê¸°ë¡ | `"Response state: TIMEOUT, resource_type: inventory.CloudService, message: Request timeout after 90s"` |
| **UNKNOWN** | WARNING | ê²½ê³  ë¡œê·¸ ê¸°ë¡ | ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœ ê°ì§€ |

### ì—ëŸ¬ ì‘ë‹µ ìë™ ë¡œê¹…

```python
from spaceone.inventory.libs.schema.cloud_service import ErrorResourceResponse

# ì—ëŸ¬ ì‘ë‹µ ìƒì„± ì‹œ ìë™ ë¡œê¹…
error_response = ErrorResourceResponse.create_with_logging(
    error_message="Connection refused to Dataproc API",
    error_code="ConnectionError", 
    resource_type="inventory.ErrorResource",
    additional_data={
        "cloud_service_group": "Dataproc",
        "cloud_service_type": "Cluster"
    }
)
# ìë™ ë¡œê·¸: "Response state: FAILURE, resource_type: inventory.ErrorResource, error_code: ConnectionError, message: Connection refused to Dataproc API"
```

## ì„±ëŠ¥ ìµœì í™”

### ì¡°ê±´ë¶€ ë¡œê¹…

```python
# âŒ í•­ìƒ ë¬¸ìì—´ ìƒì„±
logger.debug(f"Processing cluster data: {expensive_cluster_serialization()}")

# âœ… ë¡œê·¸ ë ˆë²¨ ì²´í¬ í›„ ì²˜ë¦¬
if logger.isEnabledFor(logging.DEBUG):
    logger.debug(f"Processing cluster data: {expensive_cluster_serialization()}")
```

### Google Cloud API ë¡œê¹… ìµœì í™”

```python
# API ì‘ë‹µ í¬ê¸°ê°€ í´ ê²½ìš° ìš”ì•½ë§Œ ë¡œê¹…
logger.info(f"Received {len(clusters)} clusters (total size: {sys.getsizeof(clusters)} bytes)")

# ëŒ€ì‹  ì „ì²´ ì‘ë‹µ ë¡œê¹… í”¼í•¨
# logger.debug(f"Full API response: {clusters}")  # âŒ ë„ˆë¬´ í° ë°ì´í„°
```

## ë¡œê·¸ ë³´ì•ˆ

### Google Cloud íŠ¹í™” ë¯¼ê° ë°ì´í„° ë¡œê¹… ê¸ˆì§€

**ì ˆëŒ€ ë¡œê¹…í•˜ë©´ ì•ˆ ë˜ëŠ” ê²ƒ:**

- Service Account í‚¤ íŒŒì¼ ë‚´ìš©
- ì•¡ì„¸ìŠ¤ í† í° ë° ì¸ì¦ í—¤ë”
- ì¸ìŠ¤í„´ìŠ¤ ë‚´ë¶€ IP ì£¼ì†Œ
- ë³´ì•ˆ ê·¸ë£¹ ë° ë°©í™”ë²½ ê·œì¹™ ì„¸ë¶€ì‚¬í•­
- í”„ë¡œì íŠ¸ ë²ˆí˜¸ ë° ë‚´ë¶€ ì‹ë³„ì

### ë¡œê·¸ ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ

```python
def mask_sensitive_data(message: str) -> str:
    """ë¯¼ê°í•œ ë°ì´í„°ë¥¼ ë§ˆìŠ¤í‚¹í•˜ì—¬ ë¡œê·¸ì— ì•ˆì „í•˜ê²Œ ê¸°ë¡"""
    import re
    
    # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹
    message = re.sub(r'[\w\.-]+@[\w\.-]+\.\w+', '***@***.***', message)
    
    # IP ì£¼ì†Œ ë§ˆìŠ¤í‚¹ (ë‚´ë¶€ IPë§Œ)
    message = re.sub(r'10\.\d+\.\d+\.\d+', '10.***.***.***', message)
    message = re.sub(r'192\.168\.\d+\.\d+', '192.168.***.***', message)
    
    return message

# ì‚¬ìš© ì˜ˆì‹œ
logger.info(mask_sensitive_data(f"Connected to instance {instance_info}"))
```