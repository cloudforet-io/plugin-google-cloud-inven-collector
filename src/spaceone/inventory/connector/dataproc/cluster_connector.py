import logging
import socket
import ssl
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional

import google.oauth2.service_account
import googleapiclient.discovery
from googleapiclient.errors import HttpError

from spaceone.inventory.libs.connector import GoogleCloudConnector

__all__ = ["DataprocClusterConnector"]
logger = logging.getLogger(__name__)


class DataprocClusterConnector(GoogleCloudConnector):
    google_client_service = "dataproc"
    version = "v1"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._cache_ttl = 300  # 5 minutes cache TTL
        self._regions_cache = None
        self._cache_timestamp = 0
        self._client_lock = threading.Lock()  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
        self._thread_local = threading.local()  # ìŠ¤ë ˆë“œë³„ ë…ë¦½ì ì¸ í´ë¼ì´ì–¸íŠ¸

    def verify(self, options: Dict[str, Any], secret_data: Dict[str, Any]) -> str:
        """
        ì—°ê²° ìƒíƒœë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

        Args:
            options: ê²€ì¦ ì˜µì…˜
            secret_data: Google Cloud ì¸ì¦ ì •ë³´

        Returns:
            str: ì—°ê²° ìƒíƒœ ("ACTIVE" ë˜ëŠ” "INACTIVE")

        Raises:
            Exception: ì—°ê²° ì‹¤íŒ¨ ì‹œ
        """
        try:
            self.get_connect(secret_data)
            return "ACTIVE"
        except Exception as e:
            logger.error(f"Connection verification failed: {e}")
            raise

    def get_connect(self, secret_data: Dict[str, Any]) -> None:
        """
        Google Cloud Dataprocì— ì—°ê²°ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            secret_data: Google Cloud ì¸ì¦ì„ ìœ„í•œ í¬ë¦¬ë´ì…œ
                - project_id: Google Cloud í”„ë¡œì íŠ¸ ID
                - ê¸°íƒ€ service account ì¸ì¦ì— í•„ìš”í•œ ì •ë³´

        Raises:
            ValueError: project_idê°€ ëˆ„ë½ëœ ê²½ìš°
            Exception: ì¸ì¦ ì‹¤íŒ¨ ì‹œ
        """
        if not secret_data.get("project_id"):
            raise ValueError("project_id is required in secret_data")

        self.project_id = secret_data.get("project_id")
        try:
            credentials = (
                google.oauth2.service_account.Credentials.from_service_account_info(
                    secret_data
                )
            )
            self.client = googleapiclient.discovery.build(
                "dataproc", "v1", credentials=credentials
            )
            logger.info("Successfully connected to Dataproc service")
        except ValueError as e:
            logger.error(f"Invalid service account credentials: {e}")
            raise
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"Network error during Dataproc connection: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize Dataproc connection: {e}")
            raise

    def _get_thread_safe_client(self):
        """
        ìŠ¤ë ˆë“œë³„ë¡œ ë…ë¦½ì ì¸ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            ìŠ¤ë ˆë“œë³„ ë…ë¦½ì ì¸ Google API í´ë¼ì´ì–¸íŠ¸
        """
        if (
            not hasattr(self._thread_local, "client")
            or self._thread_local.client is None
        ):
            # ê° ìŠ¤ë ˆë“œë§ˆë‹¤ ë…ë¦½ì ì¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            try:
                if hasattr(self, "credentials") and self.credentials:
                    self._thread_local.client = googleapiclient.discovery.build(
                        "dataproc",
                        "v1",
                        credentials=self.credentials,
                        cache_discovery=False,
                    )
                else:
                    # ë©”ì¸ í´ë¼ì´ì–¸íŠ¸ê°€ ìˆëŠ” ê²½ìš° í¬ë¦¬ë´ì…œì„ ì¶”ì¶œí•˜ì—¬ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    if hasattr(self, "client") and self.client:
                        # ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ì—ì„œ í¬ë¦¬ë´ì…œ ê°€ì ¸ì˜¤ê¸°
                        credentials = getattr(self.client, "_credentials", None)
                        if credentials:
                            self._thread_local.client = googleapiclient.discovery.build(
                                "dataproc",
                                "v1",
                                credentials=credentials,
                                cache_discovery=False,
                            )
                        else:
                            self._thread_local.client = self.client
                    else:
                        raise ValueError(
                            "No client or credentials available for thread-safe access"
                        )
            except Exception as e:
                logger.error(f"Failed to create thread-safe client: {e}")
                # Fallback to main client (thread-unsafe but functional)
                self._thread_local.client = getattr(self, "client", None)

        return self._thread_local.client

    def list_clusters(
        self, region: Optional[str] = None, **query: Any
    ) -> List[Dict[str, Any]]:
        """
        Dataproc í´ëŸ¬ìŠ¤í„° ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            region: í´ëŸ¬ìŠ¤í„°ë¥¼ í•„í„°ë§í•  ë¦¬ì „. Noneì¼ ê²½ìš° ëª¨ë“  ë¦¬ì „ì—ì„œ ê²€ìƒ‰
            **query: APIì— ì „ë‹¬í•  ì¶”ê°€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°

        Returns:
            í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸

        Raises:
            ValueError: í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ëœ ê²½ìš°
            HttpError: Google Cloud API ì—ëŸ¬
        """
        if not hasattr(self, "client") or not self.client:
            raise ValueError("Client not initialized. Call get_connect() first.")

        cluster_list = []

        if region:
            # íŠ¹ì • ë¦¬ì „ì˜ í´ëŸ¬ìŠ¤í„° ì¡°íšŒ
            try:
                request = (
                    self.client.projects()
                    .regions()
                    .clusters()
                    .list(projectId=self.project_id, region=region, **query)
                )
                response = request.execute()
                if "clusters" in response:
                    clusters = response.get("clusters", [])
                    cluster_list.extend(clusters)
                    logger.info(f"Found {len(clusters)} clusters in specified region")
            except HttpError as e:
                if e.resp.status == 404:
                    logger.info("No clusters found in specified region")
                else:
                    logger.error(f"HTTP error listing clusters in region: {e}")
                    raise
            except Exception as e:
                logger.error(f"Failed to list Dataproc clusters in region: {e}")
                raise
        else:
            # ëª¨ë“  ë¦¬ì „ì˜ í´ëŸ¬ìŠ¤í„° ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬)
            cluster_list = self._list_clusters_parallel(**query)

        logger.info(f"Total clusters found: {len(cluster_list)}")
        return cluster_list

    def get_cluster(self, cluster_name: str, region: str) -> Optional[Dict[str, Any]]:
        """
        íŠ¹ì • Dataproc í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            cluster_name: í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„
            region: í´ëŸ¬ìŠ¤í„°ê°€ ìœ„ì¹˜í•œ ë¦¬ì „

        Returns:
            ë°œê²¬ëœ ê²½ìš° í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ None

        Raises:
            ValueError: í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ëœ ê²½ìš°
            HttpError: Google Cloud API ì—ëŸ¬ (404 ì œì™¸)
        """
        if not cluster_name or not region:
            raise ValueError("cluster_name and region are required")

        if not hasattr(self, "client") or not self.client:
            raise ValueError("Client not initialized. Call get_connect() first.")

        try:
            request = (
                self.client.projects()
                .regions()
                .clusters()
                .get(projectId=self.project_id, region=region, clusterName=cluster_name)
            )
            cluster = request.execute()
            logger.info("Successfully retrieved cluster from region")
            return cluster
        except HttpError as e:
            if e.resp.status == 404:
                logger.info("Cluster not found in specified region")
                return None
            else:
                logger.error(f"HTTP error getting cluster in region: {e}")
                raise
        except Exception as e:
            logger.error(f"Failed to get Dataproc cluster in region: {e}")
            return None

    def list_jobs(self, region=None, cluster_name=None, **query):
        """
        Dataproc ì‘ì—… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            region (str, optional): ì‘ì—…ì„ í•„í„°ë§í•  ë¦¬ì „. Noneì¼ ê²½ìš° ëª¨ë“  ë¦¬ì „ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            cluster_name (str, optional): ì‘ì—…ì„ í•„í„°ë§í•  í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„.
            **query: APIì— ì „ë‹¬í•  ì¶”ê°€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°.

        Returns:
            list: ì‘ì—… ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        job_list = []

        # í´ëŸ¬ìŠ¤í„° í•„í„°ë§
        if cluster_name:
            query["clusterName"] = cluster_name

        if region:
            try:
                request = (
                    self.client.projects()
                    .regions()
                    .jobs()
                    .list(projectId=self.project_id, region=region, **query)
                )
                response = request.execute()
                if "jobs" in response:
                    job_list.extend(response.get("jobs", []))
            except Exception as e:
                logger.error(f"Failed to list Dataproc jobs in region: {e}")
        else:
            # ëª¨ë“  ë¦¬ì „ì˜ ì‘ì—… ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬)
            job_list = self._list_jobs_parallel(**query)

        return job_list

    def list_workflow_templates(self, region=None, **query):
        """
        Dataproc ì›Œí¬í”Œë¡œ í…œí”Œë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            region (str, optional): í…œí”Œë¦¿ì„ í•„í„°ë§í•  ë¦¬ì „. Noneì¼ ê²½ìš° ëª¨ë“  ë¦¬ì „ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            **query: APIì— ì „ë‹¬í•  ì¶”ê°€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°.

        Returns:
            list: ì›Œí¬í”Œë¡œ í…œí”Œë¦¿ ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        template_list = []

        if region:
            # íŠ¹ì • ë¦¬ì „ì˜ ì›Œí¬í”Œë¡œ í…œí”Œë¦¿ ì¡°íšŒ
            try:
                request = (
                    self.client.projects()
                    .regions()
                    .workflowTemplates()
                    .list(
                        parent=f"projects/{self.project_id}/regions/{region}", **query
                    )
                )
                response = request.execute()
                if "templates" in response:
                    template_list.extend(response.get("templates", []))
            except Exception as e:
                logger.error(
                    f"Failed to list Dataproc workflow templates in region: {e}"
                )
        else:
            # ëª¨ë“  ë¦¬ì „ì˜ ì›Œí¬í”Œë¡œ í…œí”Œë¦¿ ì¡°íšŒ
            regions = self._get_available_regions()
            for region_name in regions:
                try:
                    request = (
                        self.client.projects()
                        .regions()
                        .workflowTemplates()
                        .list(
                            parent=f"projects/{self.project_id}/regions/{region_name}",
                            **query,
                        )
                    )
                    response = request.execute()
                    if "templates" in response:
                        template_list.extend(response.get("templates", []))
                except Exception as e:
                    logger.debug(f"No Dataproc workflow templates in region: {e}")
                    continue

        return template_list

    def list_autoscaling_policies(self, region=None, **query):
        """
        Dataproc ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì •ì±… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            region (str, optional): ì •ì±…ì„ í•„í„°ë§í•  ë¦¬ì „. Noneì¼ ê²½ìš° ëª¨ë“  ë¦¬ì „ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            **query: APIì— ì „ë‹¬í•  ì¶”ê°€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°.

        Returns:
            list: ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì •ì±… ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        policy_list = []

        if region:
            # íŠ¹ì • ë¦¬ì „ì˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì •ì±… ì¡°íšŒ
            try:
                request = (
                    self.client.projects()
                    .regions()
                    .autoscalingPolicies()
                    .list(
                        parent=f"projects/{self.project_id}/regions/{region}", **query
                    )
                )
                response = request.execute()
                if "policies" in response:
                    policy_list.extend(response.get("policies", []))
            except Exception as e:
                logger.error(
                    f"Failed to list Dataproc autoscaling policies in region: {e}"
                )
        else:
            # ëª¨ë“  ë¦¬ì „ì˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì •ì±… ì¡°íšŒ
            regions = self._get_available_regions()
            for region_name in regions:
                try:
                    request = (
                        self.client.projects()
                        .regions()
                        .autoscalingPolicies()
                        .list(
                            parent=f"projects/{self.project_id}/regions/{region_name}",
                            **query,
                        )
                    )
                    response = request.execute()
                    if "policies" in response:
                        policy_list.extend(response.get("policies", []))
                except Exception as e:
                    logger.debug(f"No Dataproc autoscaling policies in region: {e}")
                    continue

        return policy_list

    def _list_clusters_parallel(self, **query) -> List[Dict[str, Any]]:
        """
        ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ëª¨ë“  ë¦¬ì „ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            **query: APIì— ì „ë‹¬í•  ì¶”ê°€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°

        Returns:
            ëª¨ë“  ë¦¬ì „ì—ì„œ ë°œê²¬ëœ í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
        """
        start_time = time.time()
        regions = self._get_optimized_regions()
        cluster_list = []

        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ ìµœì í™”)
        MAX_WORKERS = (
            2  # ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ì—ì„œ ì•ˆì •ì  ì„±ëŠ¥ì„ ìœ„í•œ ìµœì  ì„¤ì • (ì‹¤ì¸¡ í…ŒìŠ¤íŠ¸ ê²€ì¦)
        )
        max_workers = min(MAX_WORKERS, len(regions))

        # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ë¡œê¹…
        logger.info(
            f"ğŸš€ Starting parallel cluster collection: "
            f"regions={len(regions)}, max_workers={max_workers}, "
            f"global_timeout=90s, individual_timeout=60s (MAX_WORKERS={MAX_WORKERS})"
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ê° ë¦¬ì „ì— ëŒ€í•´ ë¹„ë™ê¸° ì‘ì—… ìƒì„±
            future_to_region = {
                executor.submit(self._list_clusters_in_region, region, **query): region
                for region in regions
            }

            # ì™„ë£Œëœ ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘ (ë” ê¸´ íƒ€ì„ì•„ì›ƒ)
            try:
                for future in as_completed(
                    future_to_region, timeout=90
                ):  # 90ì´ˆ íƒ€ì„ì•„ì›ƒ
                    region = future_to_region[future]
                    try:
                        clusters = future.result(timeout=60)  # ê°œë³„ ì‘ì—… 60ì´ˆ íƒ€ì„ì•„ì›ƒ
                        if clusters:
                            cluster_list.extend(clusters)
                            logger.debug(
                                f"Found {len(clusters)} clusters in region {region}"
                            )
                    except Exception as e:
                        logger.debug(f"Error processing region {region}: {e}")
                        continue
            except Exception as e:
                logger.warning(f"Timeout waiting for region processing: {e}")

        # ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ ë¡œê¹…
        execution_time = time.time() - start_time
        logger.info(
            f"âœ… Parallel cluster collection completed: "
            f"total_clusters={len(cluster_list)}, "
            f"processed_regions={len(regions)}, "
            f"execution_time={execution_time:.2f}s, "
            f"avg_time_per_region={execution_time / len(regions):.2f}s, "
            f"throughput={len(cluster_list) / execution_time:.1f} clusters/sec"
        )

        return cluster_list

    def _list_jobs_parallel(self, **query) -> List[Dict[str, Any]]:
        """
        ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ëª¨ë“  ë¦¬ì „ì˜ ì‘ì—…ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            **query: APIì— ì „ë‹¬í•  ì¶”ê°€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°

        Returns:
            ëª¨ë“  ë¦¬ì „ì—ì„œ ë°œê²¬ëœ ì‘ì—… ë¦¬ìŠ¤íŠ¸
        """
        start_time = time.time()
        regions = self._get_optimized_regions()
        job_list = []

        # ì‘ì—… ìˆ˜ì§‘ì€ í´ëŸ¬ìŠ¤í„°ë³´ë‹¤ ëœ ì¤‘ìš”í•˜ë¯€ë¡œ ë” ì ì€ ì›Œì»¤ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ ìµœì í™”)
        MAX_JOB_WORKERS = (
            1  # ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ì—ì„œ ì•ˆì •ì  ì„±ëŠ¥ì„ ìœ„í•œ ìµœì  ì„¤ì • (ì‹¤ì¸¡ í…ŒìŠ¤íŠ¸ ê²€ì¦)
        )
        max_workers = min(MAX_JOB_WORKERS, len(regions))

        # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ë¡œê¹…
        logger.info(
            f"âš¡ Starting parallel job collection: "
            f"regions={len(regions)}, max_workers={max_workers}, "
            f"individual_timeout=15s (MAX_JOB_WORKERS={MAX_JOB_WORKERS})"
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_region = {
                executor.submit(self._list_jobs_in_region, region, **query): region
                for region in regions
            }

            for future in as_completed(future_to_region):
                region = future_to_region[future]
                try:
                    jobs = future.result(
                        timeout=15
                    )  # 15ì´ˆ íƒ€ì„ì•„ì›ƒ (í´ëŸ¬ìŠ¤í„°ë³´ë‹¤ ì§§ê²Œ)
                    if jobs:
                        job_list.extend(jobs)
                except Exception as e:
                    logger.debug(f"Error processing jobs in region {region}: {e}")
                    continue

        # ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ ë¡œê¹…
        execution_time = time.time() - start_time
        logger.info(
            f"âš¡ Parallel job collection completed: "
            f"total_jobs={len(job_list)}, "
            f"processed_regions={len(regions)}, "
            f"execution_time={execution_time:.2f}s, "
            f"throughput={len(job_list) / max(execution_time, 0.001):.1f} jobs/sec"
        )

        return job_list

    def _list_jobs_in_region(self, region: str, **query) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ë¦¬ì „ì˜ ì‘ì—…ì„ ì¡°íšŒí•©ë‹ˆë‹¤ (ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨).

        Args:
            region: ì¡°íšŒí•  ë¦¬ì „ëª…
            **query: APIì— ì „ë‹¬í•  ì¶”ê°€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°

        Returns:
            í•´ë‹¹ ë¦¬ì „ì˜ ì‘ì—… ë¦¬ìŠ¤íŠ¸
        """
        max_retries = 2  # Jobì€ í´ëŸ¬ìŠ¤í„°ë³´ë‹¤ ëœ ì¤‘ìš”í•˜ë¯€ë¡œ ì¬ì‹œë„ íšŸìˆ˜ ì¶•ì†Œ
        retry_delay = 1

        for attempt in range(max_retries):
            client = None
            try:
                # ìŠ¤ë ˆë“œë³„ ë…ë¦½ì ì¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                client = self._get_thread_safe_client()
                if not client:
                    logger.warning(f"No client available for jobs in region {region}")
                    return []

                request = (
                    client.projects()
                    .regions()
                    .jobs()
                    .list(projectId=self.project_id, region=region, **query)
                )
                response = request.execute()
                return response.get("jobs", [])

            except HttpError as e:
                if e.resp.status in [404, 403]:
                    return []
                elif e.resp.status == 429 and attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    logger.debug(f"HTTP error listing jobs in region {region}: {e}")
                    return []

            except (ConnectionError, TimeoutError, socket.timeout, ssl.SSLError) as e:
                if attempt < max_retries - 1:
                    logger.debug(
                        f"Network/SSL error listing jobs in region {region}, retrying: {e}"
                    )
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    logger.debug(
                        f"Network/SSL error listing jobs in region {region}: {e}"
                    )
                    return []

            except Exception as e:
                logger.debug(f"No Dataproc jobs in region {region}: {e}")
                return []

        return []

    def _list_clusters_in_region(self, region: str, **query) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ë¦¬ì „ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° ìŠ¤ë ˆë“œ ì•ˆì „ì„± í¬í•¨).

        Args:
            region: ì¡°íšŒí•  ë¦¬ì „ëª…
            **query: APIì— ì „ë‹¬í•  ì¶”ê°€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°

        Returns:
            í•´ë‹¹ ë¦¬ì „ì˜ í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
        """
        max_retries = 3
        retry_delay = 1

        for attempt in range(max_retries):
            client = None
            try:
                # ìŠ¤ë ˆë“œë³„ ë…ë¦½ì ì¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                client = self._get_thread_safe_client()
                if not client:
                    logger.warning(f"No client available for region {region}")
                    return []

                request = (
                    client.projects()
                    .regions()
                    .clusters()
                    .list(projectId=self.project_id, region=region, **query)
                )
                response = request.execute()
                return response.get("clusters", [])

            except HttpError as e:
                if e.resp.status in [404, 403]:
                    # 404: ë¦¬ì „ì— í´ëŸ¬ìŠ¤í„° ì—†ìŒ, 403: ì ‘ê·¼ ê¶Œí•œ ì—†ìŒ
                    return []
                elif e.resp.status == 429:
                    # Rate limit - ì§€ìˆ˜ë°±ì˜¤í”„ë¡œ ëŒ€ê¸°
                    wait_time = retry_delay * (2**attempt)
                    logger.warning(
                        f"Rate limit in region {region}, waiting {wait_time}s"
                    )
                    time.sleep(wait_time)
                    continue
                elif e.resp.status >= 500:
                    # ì„œë²„ ì—ëŸ¬ - ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        logger.warning(f"Server error in region {region}, retrying...")
                        time.sleep(retry_delay * (attempt + 1))
                        continue
                else:
                    logger.warning(f"HTTP error in region {region}: {e}")
                    return []

            except (ConnectionError, TimeoutError, socket.timeout) as e:
                if attempt < max_retries - 1:
                    logger.warning(
                        f"Network error in region {region}, retrying (attempt {attempt + 1}): {e}"
                    )
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    logger.warning(
                        f"Network error in region {region} after {max_retries} attempts: {e}"
                    )
                    return []

            except ssl.SSLError as e:
                if attempt < max_retries - 1:
                    logger.warning(
                        f"SSL error in region {region}, retrying (attempt {attempt + 1}): {e}"
                    )
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    logger.warning(
                        f"SSL error in region {region} after {max_retries} attempts: {e}"
                    )
                    return []

            except Exception as e:
                # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                logger.debug(f"Unexpected error in region {region}: {e}")
                return []

        return []

    def _get_optimized_regions(self) -> List[str]:
        """
        ìµœì í™”ëœ ë¦¬ì „ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        ë™ì  ì¡°íšŒ ì‹¤íŒ¨ ì‹œ í•µì‹¬ ë¦¬ì „ë§Œ ì¡°íšŒí•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.

        Returns:
            ìµœì í™”ëœ ë¦¬ì „ ë¦¬ìŠ¤íŠ¸
        """
        current_time = time.time()

        # ìºì‹œê°€ ìœ íš¨í•œ ê²½ìš° ìºì‹œëœ ê°’ ë°˜í™˜
        if (
            self._regions_cache is not None
            and current_time - self._cache_timestamp < self._cache_ttl
        ):
            return self._regions_cache

        try:
            # ë™ì  ë¦¬ì „ ì¡°íšŒ ì‹œë„
            regions = self._fetch_dataproc_regions()
            logger.info(
                f"Successfully fetched {len(regions)} Dataproc regions dynamically"
            )
        except Exception as e:
            logger.warning(f"Failed to fetch dynamic regions, using core regions: {e}")
            # ë™ì  ì¡°íšŒ ì‹¤íŒ¨ ì‹œ í•µì‹¬ ë¦¬ì „ë§Œ ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
            regions = self._get_core_regions()

        # ìºì‹œ ì—…ë°ì´íŠ¸
        self._regions_cache = regions
        self._cache_timestamp = current_time

        logger.debug(f"Using {len(regions)} regions for Dataproc scanning")
        return regions

    def _get_core_regions(self) -> List[str]:
        """
        í•µì‹¬ ë¦¬ì „ë§Œ ë°˜í™˜í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.

        Returns:
            ì£¼ìš” ì‚¬ìš© ë¦¬ì „ ë¦¬ìŠ¤íŠ¸
        """
        return [
            # ì•„ì‹œì•„ ì£¼ìš” ë¦¬ì „
            "asia-east1",  # ëŒ€ë§Œ
            "asia-northeast1",  # ë„ì¿„
            "asia-northeast3",  # ì„œìš¸
            "asia-southeast1",  # ì‹±ê°€í¬ë¥´
            # ìœ ëŸ½ ì£¼ìš” ë¦¬ì „
            "europe-west1",  # ë²¨ê¸°ì—
            "europe-west4",  # ë„¤ëœë€ë“œ
            # ë¯¸êµ­ ì£¼ìš” ë¦¬ì „
            "us-central1",  # ì•„ì´ì˜¤ì™€
            "us-east1",  # ì‚¬ìš°ìŠ¤ ìºë¡¤ë¼ì´ë‚˜
            "us-west1",  # ì˜¤ë ˆê³¤
            "us-west2",  # ë¡œìŠ¤ì•¤ì ¤ë ˆìŠ¤
        ]

    def _get_available_regions(self) -> List[str]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ Dataproc ë¦¬ì „ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•˜ë©°, ë™ì ìœ¼ë¡œ ë¦¬ì „ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Returns:
            Dataprocì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Google Cloud ë¦¬ì „ì˜ ë¦¬ìŠ¤íŠ¸
        """
        current_time = time.time()

        # ìºì‹œê°€ ìœ íš¨í•œ ê²½ìš° ìºì‹œëœ ê°’ ë°˜í™˜
        if (
            self._regions_cache is not None
            and current_time - self._cache_timestamp < self._cache_ttl
        ):
            return self._regions_cache

        # ë™ì  ë¦¬ì „ ì¡°íšŒ ì‹œë„, ì‹¤íŒ¨ ì‹œ fallback ì‚¬ìš©
        try:
            regions = self._fetch_dataproc_regions()
            logger.info(
                f"Successfully fetched {len(regions)} Dataproc regions dynamically"
            )
        except Exception as e:
            logger.warning(f"Failed to fetch dynamic regions, using fallback: {e}")
            regions = self._get_fallback_regions()

        # ìºì‹œ ì—…ë°ì´íŠ¸
        self._regions_cache = regions
        self._cache_timestamp = current_time

        logger.debug(f"Loaded {len(regions)} available regions for Dataproc")
        return regions

    def _fetch_dataproc_regions(self) -> List[str]:
        """
        Google Cloud APIë¥¼ í†µí•´ Dataproc ì§€ì› ë¦¬ì „ì„ ë™ì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.

        Returns:
            Dataprocì„ ì§€ì›í•˜ëŠ” Google Cloud ë¦¬ì „ì˜ ë¦¬ìŠ¤íŠ¸

        Raises:
            Exception: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        if not hasattr(self, "client") or not self.client:
            raise ValueError("Client not initialized for dynamic region fetching")

        try:
            # Compute Engine APIë¥¼ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ì „ ì¡°íšŒ
            # ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ ì„¤ì •ëœ credentials ì‚¬ìš©
            compute_client = googleapiclient.discovery.build(
                "compute", "v1", credentials=self.credentials
            )
            request = compute_client.regions().list(project=self.project_id)
            response = request.execute()

            all_regions = []
            if "items" in response:
                for region in response["items"]:
                    region_name = region.get("name", "")
                    # Dataproc ì§€ì› ë¦¬ì „ í•„í„°ë§ (ì¼ë°˜ì ìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ ë¦¬ì „ì—ì„œ ì§€ì›)
                    if region_name and region.get("status") == "UP":
                        all_regions.append(region_name)

            # ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„ Dataproc ë¯¸ì§€ì› ë¦¬ì „ ì œì™¸
            excluded_regions = {"global"}
            supported_regions = [r for r in all_regions if r not in excluded_regions]

            if not supported_regions:
                raise Exception("No supported regions found")

            return sorted(supported_regions)

        except Exception as e:
            logger.error(f"Failed to fetch regions from Compute API: {e}")
            raise

    def _get_fallback_regions(self) -> List[str]:
        """
        ë™ì  ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  fallback ë¦¬ì „ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            ì•Œë ¤ì§„ Dataproc ì§€ì› ë¦¬ì „ì˜ ë¦¬ìŠ¤íŠ¸
        """
        return [
            "asia-east1",
            "asia-east2",
            "asia-northeast1",
            "asia-northeast2",
            "asia-northeast3",
            "asia-south1",
            "asia-south2",
            "asia-southeast1",
            "asia-southeast2",
            "australia-southeast1",
            "australia-southeast2",
            "europe-north1",
            "europe-west1",
            "europe-west2",
            "europe-west3",
            "europe-west4",
            "europe-west6",
            "europe-central2",
            "northamerica-northeast1",
            "northamerica-northeast2",
            "southamerica-east1",
            "southamerica-west1",
            "us-central1",
            "us-east1",
            "us-east4",
            "us-west1",
            "us-west2",
            "us-west3",
            "us-west4",
        ]
