import logging
from typing import Any, Dict, List, Tuple

from spaceone.inventory.connector.dataproc.cluster_connector import (
    DataprocClusterConnector,
)
from spaceone.inventory.libs.manager import GoogleCloudManager
from spaceone.inventory.model.dataproc.cluster.cloud_service import (
    DataprocClusterResource,
    DataprocClusterResponse,
)
from spaceone.inventory.model.dataproc.cluster.cloud_service_type import (
    CLOUD_SERVICE_TYPES,
)
from spaceone.inventory.model.dataproc.cluster.data import (
    DataprocCluster,
)

logger = logging.getLogger(__name__)


class DataprocClusterManager(GoogleCloudManager):
    connector_name = "DataprocClusterConnector"
    cloud_service_types = CLOUD_SERVICE_TYPES
    cloud_service_group = "Dataproc"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def list_clusters(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Dataproc í´ëŸ¬ìŠ¤í„° ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            params: ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°
                - secret_data: Google Cloud ì¸ì¦ ì •ë³´
                - options: ì¶”ê°€ ì˜µì…˜

        Returns:
            Dataproc í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸

        Raises:
            Exception: ì»¤ë„¥í„° ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        """
        if not params or "secret_data" not in params:
            raise ValueError("secret_data is required in params")

        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            clusters = cluster_connector.list_clusters()
            logger.info(
                f"ðŸ“Š Successfully found {len(clusters)} Dataproc clusters "
                f"(parallel processing enabled)"
            )
            return clusters
        except Exception as e:
            logger.error(f"Failed to list Dataproc clusters: {e}")
            return []

    def get_cluster(
        self, cluster_name: str, region: str, params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        íŠ¹ì • Dataproc í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            cluster_name (str): í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„.
            region (str): í´ëŸ¬ìŠ¤í„°ê°€ ìœ„ì¹˜í•œ ë¦¬ì „.
            params (dict): ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°.

        Returns:
            dict: ë°œê²¬ëœ ê²½ìš° í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬.
        """
        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            cluster = cluster_connector.get_cluster(cluster_name, region)
            if cluster:
                logger.info("Retrieved Dataproc cluster successfully")
            return cluster or {}
        except Exception as e:
            logger.error(f"Failed to get Dataproc cluster: {e}")
            return {}

    def list_jobs(
        self,
        region: str = None,
        cluster_name: str = None,
        params: Dict[str, Any] = None,
    ) -> List[Dict[str, Any]]:
        """
        Dataproc ìž‘ì—… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            region (str, optional): ìž‘ì—…ì„ í•„í„°ë§í•  ë¦¬ì „.
            cluster_name (str, optional): ìž‘ì—…ì„ í•„í„°ë§í•  í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„.
            params (dict, optional): ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°.

        Returns:
            list: Dataproc ìž‘ì—… ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        if params is None:
            params = {}

        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            jobs = cluster_connector.list_jobs(region=region, cluster_name=cluster_name)
            logger.info(
                f"âš¡ Found {len(jobs)} Dataproc jobs "
                f"(parallel processing with optimized timeouts)"
            )
            return jobs
        except Exception as e:
            logger.error(f"Failed to list Dataproc jobs: {e}")
            return []

    def list_workflow_templates(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Dataproc ì›Œí¬í”Œë¡œ í…œí”Œë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            params (dict): ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°.

        Returns:
            list: Dataproc ì›Œí¬í”Œë¡œ í…œí”Œë¦¿ ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            templates = cluster_connector.list_workflow_templates()
            logger.info(f"Found {len(templates)} Dataproc workflow templates")
            return templates
        except Exception as e:
            logger.error(f"Failed to list Dataproc workflow templates: {e}")
            return []

    def list_autoscaling_policies(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Dataproc ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì •ì±… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            params (dict): ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°.

        Returns:
            list: Dataproc ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì •ì±… ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            policies = cluster_connector.list_autoscaling_policies()
            logger.info(f"Found {len(policies)} Dataproc autoscaling policies")
            return policies
        except Exception as e:
            logger.error(f"Failed to list Dataproc autoscaling policies: {e}")
            return []

    def collect_cloud_service(
        self, params: Dict[str, Any]
    ) -> Tuple[List[DataprocClusterResponse], List[Dict[str, Any]]]:
        """
        Dataproc í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Cloud Service ë¦¬ì†ŒìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            params: ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„°
                - secret_data: Google Cloud ì¸ì¦ ì •ë³´
                - options: ì¶”ê°€ ìˆ˜ì§‘ ì˜µì…˜

        Returns:
            ìˆ˜ì§‘ëœ Cloud Service ì‘ë‹µ ë¦¬ìŠ¤íŠ¸ì™€ ì—ëŸ¬ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸ì˜ íŠœí”Œ

        Raises:
            ValueError: í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ëœ ê²½ìš°
        """
        logger.debug("** Dataproc Cluster START **")

        if not params or "secret_data" not in params:
            raise ValueError("secret_data is required in params")

        collected_cloud_services = []
        error_responses = []

        secret_data = params["secret_data"]
        project_id = secret_data.get("project_id")

        if not project_id:
            raise ValueError("project_id is required in secret_data")

        # Dataproc í´ëŸ¬ìŠ¤í„° ëª©ë¡ ì¡°íšŒ
        try:
            clusters = self.list_clusters(params)
            if not clusters:
                logger.info("No Dataproc clusters found")
                return collected_cloud_services, error_responses
        except Exception as e:
            logger.error(f"Failed to retrieve cluster list: {e}")
            error_responses.append(
                self.generate_error_response(e, self.cloud_service_group, "Cluster")
            )
            return collected_cloud_services, error_responses

        for cluster in clusters:
            try:
                # í´ëŸ¬ìŠ¤í„° ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                location = ""
                if "placement" in cluster and "zoneUri" in cluster["placement"]:
                    zone_uri = cluster["placement"]["zoneUri"]
                    location = zone_uri.split("/")[-1] if zone_uri else ""
                elif "config" in cluster and "gceClusterConfig" in cluster["config"]:
                    # zone ì •ë³´ê°€ ìžˆìœ¼ë©´ í•´ë‹¹ ì§€ì—­ì„ ì¶”ì¶œ
                    zone_uri = cluster["config"]["gceClusterConfig"].get("zoneUri", "")
                    if zone_uri:
                        location = zone_uri.split("/")[-1]

                # í´ëŸ¬ìŠ¤í„°ëª… ì¶”ì¶œ
                cluster_name = cluster.get("clusterName", "")

                # ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ë°ì´í„° ì¤€ë¹„
                cluster_data = {
                    "clusterName": str(cluster.get("clusterName", "")),
                    "projectId": str(cluster.get("projectId", project_id)),
                    "clusterUuid": str(cluster.get("clusterUuid", "")),
                    "status": cluster.get("status", {}),
                    "labels": {k: str(v) for k, v in cluster.get("labels", {}).items()},
                    "location": location,
                }

                # ì„¤ì • ì •ë³´ ì¶”ê°€
                if "config" in cluster:
                    config = cluster["config"]
                    cluster_data["config"] = {
                        "configBucket": str(config.get("configBucket", "")),
                        "tempBucket": str(config.get("tempBucket", "")),
                    }

                    # GCE í´ëŸ¬ìŠ¤í„° ì„¤ì •
                    if "gceClusterConfig" in config:
                        gce_config = config["gceClusterConfig"]
                        cluster_data["config"]["gceClusterConfig"] = {
                            "zoneUri": str(gce_config.get("zoneUri", "")),
                            "networkUri": str(gce_config.get("networkUri", "")),
                            "subnetworkUri": str(gce_config.get("subnetworkUri", "")),
                            "internalIpOnly": str(gce_config.get("internalIpOnly", "")),
                            "serviceAccount": str(gce_config.get("serviceAccount", "")),
                            "serviceAccountScopes": gce_config.get(
                                "serviceAccountScopes", []
                            ),
                        }

                    # ì¸ìŠ¤í„´ìŠ¤ ê·¸ë£¹ ì„¤ì •
                    if "instanceGroupConfig" in config:
                        instance_config = config["instanceGroupConfig"]
                        cluster_data["config"]["instanceGroupConfig"] = {
                            "numInstances": str(
                                instance_config.get("numInstances", "")
                            ),
                            "instanceNames": instance_config.get("instanceNames", []),
                            "imageUri": str(instance_config.get("imageUri", "")),
                            "machineTypeUri": str(
                                instance_config.get("machineTypeUri", "")
                            ),
                            "diskConfig": instance_config.get("diskConfig", {}),
                        }

                    # ë§ˆìŠ¤í„° ì„¤ì •
                    if "masterConfig" in config:
                        master_config = config["masterConfig"]
                        cluster_data["config"]["masterConfig"] = {
                            "numInstances": str(master_config.get("numInstances", "")),
                            "instanceNames": master_config.get("instanceNames", []),
                            "imageUri": str(master_config.get("imageUri", "")),
                            "machineTypeUri": str(
                                master_config.get("machineTypeUri", "")
                            ),
                            "diskConfig": master_config.get("diskConfig", {}),
                        }

                    # ì›Œì»¤ ì„¤ì •
                    if "workerConfig" in config:
                        worker_config = config["workerConfig"]
                        cluster_data["config"]["workerConfig"] = {
                            "numInstances": str(worker_config.get("numInstances", "")),
                            "instanceNames": worker_config.get("instanceNames", []),
                            "imageUri": str(worker_config.get("imageUri", "")),
                            "machineTypeUri": str(
                                worker_config.get("machineTypeUri", "")
                            ),
                            "diskConfig": worker_config.get("diskConfig", {}),
                        }

                    # ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì •
                    if "softwareConfig" in config:
                        software_config = config["softwareConfig"]
                        cluster_data["config"]["softwareConfig"] = {
                            "imageVersion": str(
                                software_config.get("imageVersion", "")
                            ),
                            "properties": software_config.get("properties", {}),
                            "optionalComponents": software_config.get(
                                "optionalComponents", []
                            ),
                        }

                # ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ê°€
                if "metrics" in cluster:
                    cluster_data["metrics"] = cluster["metrics"]

                # Job ì •ë³´ ìˆ˜ì§‘ ìµœì í™” - ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì„ íƒì ìœ¼ë¡œ ìˆ˜ì§‘
                cluster_data["jobs"] = []
                # Job ìˆ˜ì§‘ì€ ë³„ë„ ì˜µì…˜ì´ ìžˆì„ ë•Œë§Œ ìˆ˜í–‰ (ì„±ëŠ¥ ìµœì í™”)
                if params.get("options", {}).get("include_jobs", False):
                    try:
                        # í´ëŸ¬ìŠ¤í„° ìœ„ì¹˜ì—ì„œ ë¦¬ì „ ì¶”ì¶œ
                        cluster_region = (
                            location.rsplit("-", 1)[0]
                            if location and "-" in location
                            else location
                        )
                        if cluster_region:
                            jobs = self.list_jobs(
                                region=cluster_region,
                                cluster_name=cluster_name,
                                params=params,
                            )
                            if jobs:
                                # ìµœê·¼ ìž‘ì—… ìˆ˜ì§‘ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì œí•œ)
                                job_limit = min(5, len(jobs))  # ìµœëŒ€ 5ê°œë¡œ ì¶•ì†Œ
                                for job in jobs[:job_limit]:
                                    job_data = {
                                        "reference": job.get("reference", {}),
                                        "placement": job.get("placement", {}),
                                        "status": job.get("status", {}),
                                        "labels": job.get("labels", {}),
                                        "jobUuid": job.get("jobUuid", ""),
                                    }
                                    cluster_data["jobs"].append(job_data)
                    except Exception as e:
                        logger.warning(f"Failed to collect jobs for cluster: {e}")
                        # jobsëŠ” ì´ë¯¸ ë¹ˆ ë°°ì—´ë¡œ ì´ˆê¸°í™”ë¨
                else:
                    # Job ìˆ˜ì§‘ ìƒëžµ - ì„±ëŠ¥ ìµœì í™”
                    logger.debug("Job collection skipped for performance optimization")

                # DataprocCluster ëª¨ë¸ ìƒì„±
                dataproc_cluster_data = DataprocCluster(cluster_data, strict=False)

                # DataprocClusterResource ìƒì„±
                cluster_resource = DataprocClusterResource(
                    {
                        "name": cluster_data.get("clusterName"),
                        "data": dataproc_cluster_data,
                        "reference": {
                            "resource_id": cluster.get("clusterUuid"),
                            "external_link": f"https://console.cloud.google.com/dataproc/clusters/details/{location}/{cluster_name}?project={project_id}",
                        },
                        "region_code": location,
                        "account": project_id,
                    }
                )

                ##################################
                # 4. Make Collected Region Code
                ##################################
                self.set_region_code(location)

                # DataprocClusterResponse ìƒì„±
                cluster_response = DataprocClusterResponse(
                    {"resource": cluster_resource}
                )

                collected_cloud_services.append(cluster_response)

            except Exception as e:
                logger.error(f"[collect_cloud_service] => {e}", exc_info=True)
                error_responses.append(
                    self.generate_error_response(e, self.cloud_service_group, "Cluster")
                )

        logger.debug("** Dataproc Cluster END **")
        return collected_cloud_services, error_responses
