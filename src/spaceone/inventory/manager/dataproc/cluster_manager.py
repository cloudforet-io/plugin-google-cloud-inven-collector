import logging
from typing import Any, Dict, List, Tuple

from spaceone.inventory.connector.dataproc.cluster_connector import (
    DataprocClusterConnector,
)
from spaceone.inventory.libs.manager import GoogleCloudManager
from spaceone.inventory.model.dataproc.cluster.cloud_service import (
    DataprocClusterResource,
    DataprocClusterResponse,
)
from spaceone.inventory.model.dataproc.cluster.cloud_service_type import (
    CLOUD_SERVICE_TYPES,
)
from spaceone.inventory.model.dataproc.cluster.data import (
    DataprocCluster,
)

logger = logging.getLogger(__name__)


class DataprocClusterManager(GoogleCloudManager):
    connector_name = "DataprocClusterConnector"
    cloud_service_types = CLOUD_SERVICE_TYPES
    cloud_service_group = "Dataproc"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def list_clusters(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Dataproc í´ëŸ¬ìŠ¤í„° ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            params: ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°
                - secret_data: Google Cloud ì¸ì¦ ì •ë³´
                - options: ì¶”ê°€ ì˜µì…˜

        Returns:
            Dataproc í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸

        Raises:
            Exception: ì»¤ë„¥í„° ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        """
        if not params or "secret_data" not in params:
            raise ValueError("secret_data is required in params")

        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            clusters = cluster_connector.list_clusters()
            logger.info(
                f"ðŸ“Š Successfully found {len(clusters)} Dataproc clusters "
                f"(parallel processing enabled)"
            )
            return clusters
        except Exception as e:
            logger.error(f"Failed to list Dataproc clusters: {e}")
            return []

    def get_cluster(
        self, cluster_name: str, region: str, params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        íŠ¹ì • Dataproc í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            cluster_name (str): í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„.
            region (str): í´ëŸ¬ìŠ¤í„°ê°€ ìœ„ì¹˜í•œ ë¦¬ì „.
            params (dict): ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°.

        Returns:
            dict: ë°œê²¬ëœ ê²½ìš° í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬.
        """
        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            cluster = cluster_connector.get_cluster(cluster_name, region)
            if cluster:
                logger.info("Retrieved Dataproc cluster successfully")
            return cluster or {}
        except Exception as e:
            logger.error(f"Failed to get Dataproc cluster: {e}")
            return {}

    def list_jobs(
        self,
        region: str = None,
        cluster_name: str = None,
        params: Dict[str, Any] = None,
    ) -> List[Dict[str, Any]]:
        """
        Dataproc ìž‘ì—… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            region (str, optional): ìž‘ì—…ì„ í•„í„°ë§í•  ë¦¬ì „.
            cluster_name (str, optional): ìž‘ì—…ì„ í•„í„°ë§í•  í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„.
            params (dict, optional): ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°.

        Returns:
            list: Dataproc ìž‘ì—… ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        if params is None:
            params = {}

        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            jobs = cluster_connector.list_jobs(region=region, cluster_name=cluster_name)
            logger.info(
                f"âš¡ Found {len(jobs)} Dataproc jobs "
                f"(parallel processing with optimized timeouts)"
            )
            return jobs
        except Exception as e:
            logger.error(f"Failed to list Dataproc jobs: {e}")
            return []

    def list_workflow_templates(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Dataproc ì›Œí¬í”Œë¡œ í…œí”Œë¦¿ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            params (dict): ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°.

        Returns:
            list: Dataproc ì›Œí¬í”Œë¡œ í…œí”Œë¦¿ ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            templates = cluster_connector.list_workflow_templates()
            logger.info(f"Found {len(templates)} Dataproc workflow templates")
            return templates
        except Exception as e:
            logger.error(f"Failed to list Dataproc workflow templates: {e}")
            return []

    def list_autoscaling_policies(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Dataproc ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì •ì±… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            params (dict): ì»¤ë„¥í„°ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°.

        Returns:
            list: Dataproc ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì •ì±… ë¦¬ì†ŒìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        cluster_connector: DataprocClusterConnector = self.locator.get_connector(
            self.connector_name, **params
        )

        try:
            policies = cluster_connector.list_autoscaling_policies()
            logger.info(f"Found {len(policies)} Dataproc autoscaling policies")
            return policies
        except Exception as e:
            logger.error(f"Failed to list Dataproc autoscaling policies: {e}")
            return []

    def collect_cloud_service(
        self, params: Dict[str, Any]
    ) -> Tuple[List[DataprocClusterResponse], List[Dict[str, Any]]]:
        """
        Dataproc í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Cloud Service ë¦¬ì†ŒìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            params: ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„°
                - secret_data: Google Cloud ì¸ì¦ ì •ë³´
                - options: ì¶”ê°€ ìˆ˜ì§‘ ì˜µì…˜

        Returns:
            ìˆ˜ì§‘ëœ Cloud Service ì‘ë‹µ ë¦¬ìŠ¤íŠ¸ì™€ ì—ëŸ¬ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸ì˜ íŠœí”Œ

        Raises:
            ValueError: í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ëœ ê²½ìš°
        """
        logger.debug("** Dataproc Cluster START **")

        if not params or "secret_data" not in params:
            raise ValueError("secret_data is required in params")

        collected_cloud_services = []
        error_responses = []

        secret_data = params["secret_data"]
        project_id = secret_data.get("project_id")

        if not project_id:
            raise ValueError("project_id is required in secret_data")

        # Dataproc í´ëŸ¬ìŠ¤í„° ëª©ë¡ ì¡°íšŒ
        try:
            clusters = self.list_clusters(params)
            if not clusters:
                logger.info("No Dataproc clusters found")
                return collected_cloud_services, error_responses
        except Exception as e:
            logger.error(f"Failed to retrieve cluster list: {e}")
            error_responses.append(
                self.generate_error_response(e, self.cloud_service_group, "Cluster")
            )
            return collected_cloud_services, error_responses

        for cluster in clusters:
            try:
                # í´ëŸ¬ìŠ¤í„° ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                location = ""
                if "placement" in cluster and "zoneUri" in cluster["placement"]:
                    zone_uri = cluster["placement"]["zoneUri"]
                    location = zone_uri.split("/")[-1] if zone_uri else ""
                elif "config" in cluster and "gceClusterConfig" in cluster["config"]:
                    # zone ì •ë³´ê°€ ìžˆìœ¼ë©´ í•´ë‹¹ ì§€ì—­ì„ ì¶”ì¶œ
                    zone_uri = cluster["config"]["gceClusterConfig"].get("zoneUri", "")
                    if zone_uri:
                        location = zone_uri.split("/")[-1]

                # í´ëŸ¬ìŠ¤í„°ëª… ì¶”ì¶œ
                cluster_name = cluster.get("clusterName", "")

                # ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ë°ì´í„° ì¤€ë¹„
                cluster_data = {
                    "name": str(cluster.get("clusterName", "")),  # name í•„ë“œë¡œ ë§¤í•‘
                    "cluster_name": str(cluster.get("clusterName", "")),
                    "project_id": str(project_id),  # project_idë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
                    "cluster_uuid": str(cluster.get("clusterUuid", "")),
                    "status": cluster.get("status", {}),
                    "labels": {k: str(v) for k, v in cluster.get("labels", {}).items()},
                    "location": location,
                }

                # ì„¤ì • ì •ë³´ ì¶”ê°€
                config = cluster.get("config", {})
                cluster_data["config"] = {
                    "config_bucket": str(config.get("configBucket", "")),
                    "temp_bucket": str(config.get("tempBucket", "")),
                }

                # GCE í´ëŸ¬ìŠ¤í„° ì„¤ì •
                if "gceClusterConfig" in config:
                    gce_config = config["gceClusterConfig"]
                    cluster_data["config"]["gce_cluster_config"] = {
                        "zone_uri": str(gce_config.get("zoneUri", "")),
                        "network_uri": str(gce_config.get("networkUri", "")),
                        "subnetwork_uri": str(gce_config.get("subnetworkUri", "")),
                        "internal_ip_only": str(gce_config.get("internalIpOnly", "")),
                        "service_account": str(gce_config.get("serviceAccount", "")),
                        "service_account_scopes": gce_config.get(
                            "serviceAccountScopes", []
                        ),
                    }

                # ì¸ìŠ¤í„´ìŠ¤ ê·¸ë£¹ ì„¤ì •
                if "instanceGroupConfig" in config:
                    instance_config = config["instanceGroupConfig"]
                    cluster_data["config"]["instanceGroupConfig"] = {
                        "numInstances": str(instance_config.get("numInstances", "")),
                        "instanceNames": instance_config.get("instanceNames", []),
                        "imageUri": str(instance_config.get("imageUri", "")),
                        "machineTypeUri": str(
                            instance_config.get("machineTypeUri", "")
                        ),
                        "diskConfig": instance_config.get("diskConfig", {}),
                    }

                # ë§ˆìŠ¤í„° ì„¤ì •
                master_config = config.get("masterConfig", {})
                if master_config:
                    cluster_data["config"]["master_config"] = {
                        "num_instances": str(master_config.get("numInstances", "")),
                        "instance_names": master_config.get("instanceNames", []),
                        "image_uri": str(master_config.get("imageUri", "")),
                        "machine_type_uri": str(
                            master_config.get("machineTypeUri", "")
                        ),
                        "disk_config": master_config.get("diskConfig", {}),
                        "preemptibility": str(
                            master_config.get("preemptibility", "NON_PREEMPTIBLE")
                        ),
                    }
                else:
                    cluster_data["config"]["master_config"] = {
                        "num_instances": "",
                        "instance_names": [],
                        "image_uri": "",
                        "machine_type_uri": "",
                        "disk_config": {},
                        "preemptibility": "NON_PREEMPTIBLE",
                    }

                # ì›Œì»¤ ì„¤ì •
                worker_config = config.get("workerConfig", {})
                if worker_config:
                    cluster_data["config"]["worker_config"] = {
                        "num_instances": str(worker_config.get("numInstances", "")),
                        "instance_names": worker_config.get("instanceNames", []),
                        "image_uri": str(worker_config.get("imageUri", "")),
                        "machine_type_uri": str(
                            worker_config.get("machineTypeUri", "")
                        ),
                        "disk_config": worker_config.get("diskConfig", {}),
                    }
                else:
                    cluster_data["config"]["worker_config"] = {
                        "num_instances": "",
                        "instance_names": [],
                        "image_uri": "",
                        "machine_type_uri": "",
                        "disk_config": {},
                    }

                # ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì •
                software_config = config.get("softwareConfig", {})
                if software_config:
                    cluster_data["config"]["software_config"] = {
                        "image_version": str(software_config.get("imageVersion", "")),
                        "properties": software_config.get("properties", {}),
                        "optional_components": software_config.get(
                            "optionalComponents", []
                        ),
                    }
                else:
                    cluster_data["config"]["software_config"] = {
                        "image_version": "",
                        "properties": {},
                        "optional_components": [],
                    }

                # Secondary Worker Config (Preemptible VMs)
                secondary_worker_config = config.get("secondaryWorkerConfig", {})
                if secondary_worker_config:
                    cluster_data["config"]["secondary_worker_config"] = {
                        "num_instances": str(
                            secondary_worker_config.get("numInstances", "")
                        ),
                        "instance_names": secondary_worker_config.get(
                            "instanceNames", []
                        ),
                        "image_uri": str(secondary_worker_config.get("imageUri", "")),
                        "machine_type_uri": str(
                            secondary_worker_config.get("machineTypeUri", "")
                        ),
                        "disk_config": secondary_worker_config.get("diskConfig", {}),
                    }
                else:
                    cluster_data["config"]["secondary_worker_config"] = {
                        "num_instances": "",
                        "instance_names": [],
                        "image_uri": "",
                        "machine_type_uri": "",
                        "disk_config": {},
                    }

                # Lifecycle Config (Scheduled Deletion)
                lifecycle_config = config.get("lifecycleConfig", {})
                if lifecycle_config:
                    cluster_data["config"]["lifecycle_config"] = {
                        "auto_delete_time": str(
                            lifecycle_config.get("autoDeleteTime", "")
                        ),
                        "auto_delete_ttl": str(
                            lifecycle_config.get("autoDeleteTtl", "")
                        ),
                        "idle_delete_ttl": str(
                            lifecycle_config.get("idleDeleteTtl", "")
                        ),
                    }
                else:
                    cluster_data["config"]["lifecycle_config"] = {
                        "auto_delete_time": "",
                        "auto_delete_ttl": "",
                        "idle_delete_ttl": "",
                    }

                # ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ê°€
                if "metrics" in cluster:
                    cluster_data["metrics"] = cluster["metrics"]

                # Job ì •ë³´ ìˆ˜ì§‘ ìµœì í™” - ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì„ íƒì ìœ¼ë¡œ ìˆ˜ì§‘
                cluster_data["jobs"] = []
                # Job ìˆ˜ì§‘ì€ ë³„ë„ ì˜µì…˜ì´ ìžˆì„ ë•Œë§Œ ìˆ˜í–‰ (ì„±ëŠ¥ ìµœì í™”)
                if params.get("options", {}).get("include_jobs", False):
                    try:
                        # í´ëŸ¬ìŠ¤í„° ìœ„ì¹˜ì—ì„œ ë¦¬ì „ ì¶”ì¶œ
                        cluster_region = (
                            location.rsplit("-", 1)[0]
                            if location and "-" in location
                            else location
                        )
                        if cluster_region:
                            jobs = self.list_jobs(
                                region=cluster_region,
                                cluster_name=cluster_name,
                                params=params,
                            )
                            if jobs:
                                # ìµœê·¼ ìž‘ì—… ìˆ˜ì§‘ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì œí•œ)
                                job_limit = min(5, len(jobs))  # ìµœëŒ€ 5ê°œë¡œ ì¶•ì†Œ
                                for job in jobs[:job_limit]:
                                    job_data = {
                                        "reference": job.get("reference", {}),
                                        "placement": job.get("placement", {}),
                                        "status": job.get("status", {}),
                                        "labels": job.get("labels", {}),
                                        "jobUuid": job.get("jobUuid", ""),
                                    }
                                    cluster_data["jobs"].append(job_data)
                    except Exception as e:
                        logger.warning(f"Failed to collect jobs for cluster: {e}")
                        # jobsëŠ” ì´ë¯¸ ë¹ˆ ë°°ì—´ë¡œ ì´ˆê¸°í™”ë¨
                else:
                    # Job ìˆ˜ì§‘ ìƒëžµ - ì„±ëŠ¥ ìµœì í™”
                    logger.debug("Job collection skipped for performance optimization")

                # DataprocCluster ëª¨ë¸ ìƒì„±
                dataproc_cluster_data = DataprocCluster(cluster_data, strict=False)

                # DataprocClusterResource ìƒì„±
                cluster_resource = DataprocClusterResource(
                    {
                        "name": cluster_data.get("name"),
                        "data": dataproc_cluster_data,
                        "reference": {
                            "resource_id": cluster.get("clusterUuid"),
                            "external_link": f"https://console.cloud.google.com/dataproc/clusters/details/{location}/{cluster_name}?project={project_id}",
                        },
                        "region_code": location,
                        "account": project_id,
                    }
                )

                ##################################
                # 4. Make Collected Region Code
                ##################################
                self.set_region_code(location)

                # DataprocClusterResponse ìƒì„±
                cluster_response = DataprocClusterResponse(
                    {"resource": cluster_resource}
                )

                collected_cloud_services.append(cluster_response)

            except Exception as e:
                logger.error(f"[collect_cloud_service] => {e}", exc_info=True)
                error_responses.append(
                    self.generate_error_response(e, self.cloud_service_group, "Cluster")
                )

        logger.debug("** Dataproc Cluster END **")
        return collected_cloud_services, error_responses
